application {
    name = Spot DNS ingest 
    batch.milliseconds = 5000
    executors = 1
    executor.cores = 1
    executor.memory = 1G
    spark.conf.hive.exec.dynamic.partition = true
    spark.conf.hive.exec.dynamic.partition.mode = nonstrict 
}

steps {
    dns_received {
        input {
            type = kafka
            brokers = "cjh-1.vpc.cloudera.com:9092"
            topic = traffic
            encoding = string
            translator {
                type = delimited
                delimiter = ","
                field.names = [event_time1,event_time2,epoch,dns_len,src_ip4_str,dst_ip4_str,dns_query,dns_type,dns_class,dns_response_code,dns_answers]
                field.types = [string,string,float,int,string,string,string,int,string,string,string]
            }
        }
    }

   spot_event_input {
       input {
           type = hive
           table = "spot.event"
       }
   }

   spot_event {
       dependencies = [spot_event_input]
       deriver {
           type = sql
           query.literal = "SELECT * from spot_event_input limit 1"
           hint.small = true
           cache.enabled = true
       }
   }
 
    dns_formatted {
        dependencies = [dns_received, spot_event_input]
        deriver {
            align.schema = spot_event_input 
            type = sql
            query.literal = """
                SELECT 
                  unix_timestamp(concat(event_time1, event_time2), "MMM dd yyyy HH:mm:ss") as event_time
                FROM dns_received"""
        }
        planner {
            type = append 
        }
        output {
            type = filesystem 
            path = "/user/hdfs/test.csv"
            format = json
        }
    }
}
