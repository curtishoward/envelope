application {
    name = Spot DNS ingest
    batch.milliseconds = 5000
    executors = 1
    executor.cores = 1
    executor.memory = 1G
    spark.conf.hive.exec.dynamic.partition = true
    spark.conf.hive.exec.dynamic.partition.mode = nonstrict
}

steps {
    dns_received {
        input {
            type = kafka
            brokers = "cjh-1.vpc.cloudera.com:9092"
            topic = spot_dns
            encoding = string
            translator {
                type = delimited
                delimiter = ","
                field.names = [event_time1,event_time2,epoch,dns_len,src_ip4_str,dst_ip4_str,dns_query,dns_type,dns_class,dns_response_code,dns_answers]
                field.types = [string,string,float,int,string,string,string,int,string,string,string]
            }
        }
    }

    dns_formatted {
        dependencies = [dns_received]
        deriver {
            type = sql
            query.literal = """
                SELECT
                  unix_timestamp(concat(event_time1, event_time2), "MMM dd yyyy HH:mm:ss") as event_time,
                  src_ip4_str,
                  dst_ip4_str,
                  dns_class,
                  dns_len,
                  dns_query,
                  dns_response_code,
                  dns_answers,
                  dns_type,
                  'SOME_VENDOR' as p_dvc_vendor,
                  'SOME_DEVICE_TYPE' as p_dvc_type,
                  FROM_UNIXTIME(unix_timestamp(concat(event_time1, event_time2), "MMM dd yyyy HH:mm:ss"), "yyyy-MM-dd") as p_dt
                FROM dns_received"""
        }
        planner {
            type = append
        }
        output {
            type = hive
            table = "spot.event"
            infer.columns = true
        }
    }
}
